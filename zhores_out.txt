0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
[2023-04-04 14-45-02] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Task                : "check"
Kind of task        : "data"
=========================================


.... Loading "cifar10" dataset
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /beegfs/home/a.chertkov/neural_tensor_train/data/_data/cifar-10-python.tar.gz
Extracting /beegfs/home/a.chertkov/neural_tensor_train/data/_data/cifar-10-python.tar.gz to /beegfs/home/a.chertkov/neural_tensor_train/data/_data
Files already downloaded and verified
DONE (    22.37 sec.) 


.... Check data for "cifar10" dataset
Dataset             : cifar10
Number of classes   :         10
Size of trn dataset :      50000
Size of tst dataset :      10000
Var  of trn dataset : 6.3287e-02
Var  of tst dataset : 6.3111e-02

DONE (     4.87 sec.) 


===================== -----------------------------------
[2023-04-04 14-45-30] Work is finished (27.25 sec. total)


[2023-04-04 14-45-54] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Model               : "densenet"
Task                : "check"
Kind of task        : "model"
Target class        : "0"
=========================================


.... Loading "cifar10" dataset
DONE (     5.36 sec.) 


.... Loading "densenet" model
DONE (    27.63 sec.) 

Accuracy   trn : 99.94% (    49968 /     50000) | time =      82.44 sec
Activation trn : [5.0e-04, 9.8e-01] (avg: 1.0e-01)

Accuracy   tst : 94.07% (     9407 /     10000) | time =      13.21 sec
Activation tst : [1.0e-03, 9.8e-01] (avg: 1.0e-01)



===================== ------------------------------------
[2023-04-04 14-48-04] Work is finished (130.46 sec. total)


[2023-04-04 14-48-25] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "train"
Kind of task        : "gen"
=========================================


.... Loading "cifar10" dataset
DONE (     5.34 sec.) 


.... Loading "vae_vq" generator
DONE (     7.12 sec.) 


.... Loading "densenet" model
DONE (     2.05 sec.) 


.... Training "vae_vq_cifar10" model
#        1 | time 1.5e+01 sec | E recon = 1.646e+01 | Perplexity = 1.008e+00 | 
#      500 | time 5.0e+01 sec | E recon = 7.707e+00 | Perplexity = 1.982e+00 | 
#     1000 | time 8.5e+01 sec | E recon = 2.748e+00 | Perplexity = 2.491e+01 | 
#     1500 | time 1.2e+02 sec | E recon = 1.714e+00 | Perplexity = 6.687e+01 | 
#     2000 | time 1.6e+02 sec | E recon = 1.460e+00 | Perplexity = 1.057e+02 | 
#     2500 | time 1.9e+02 sec | E recon = 1.285e+00 | Perplexity = 1.509e+02 | 
#     3000 | time 2.3e+02 sec | E recon = 1.179e+00 | Perplexity = 2.062e+02 | 
#     3500 | time 2.6e+02 sec | E recon = 1.089e+00 | Perplexity = 2.786e+02 | 
#     4000 | time 3.0e+02 sec | E recon = 1.018e+00 | Perplexity = 3.558e+02 | 
#     4500 | time 3.4e+02 sec | E recon = 9.733e-01 | Perplexity = 3.945e+02 | 
#     5000 | time 3.7e+02 sec | E recon = 9.414e-01 | Perplexity = 4.061e+02 | 
#     5500 | time 4.1e+02 sec | E recon = 9.230e-01 | Perplexity = 4.079e+02 | 
#     6000 | time 4.5e+02 sec | E recon = 9.069e-01 | Perplexity = 4.082e+02 | 
#     6500 | time 4.8e+02 sec | E recon = 8.944e-01 | Perplexity = 4.079e+02 | 
#     7000 | time 5.2e+02 sec | E recon = 8.847e-01 | Perplexity = 4.069e+02 | 
#     7500 | time 5.5e+02 sec | E recon = 8.734e-01 | Perplexity = 4.059e+02 | 
#     8000 | time 5.9e+02 sec | E recon = 8.689e-01 | Perplexity = 4.056e+02 | 
#     8500 | time 6.2e+02 sec | E recon = 8.629e-01 | Perplexity = 4.058e+02 | 
#     9000 | time 6.6e+02 sec | E recon = 8.592e-01 | Perplexity = 4.053e+02 | 
#     9500 | time 6.9e+02 sec | E recon = 8.535e-01 | Perplexity = 4.059e+02 | 
#    10000 | time 7.3e+02 sec | E recon = 8.460e-01 | Perplexity = 4.051e+02 | 
#    10500 | time 7.7e+02 sec | E recon = 8.454e-01 | Perplexity = 4.061e+02 | 
#    11000 | time 8.0e+02 sec | E recon = 8.431e-01 | Perplexity = 4.055e+02 | 
#    11500 | time 8.4e+02 sec | E recon = 8.402e-01 | Perplexity = 4.053e+02 | 
#    12000 | time 8.7e+02 sec | E recon = 8.338e-01 | Perplexity = 4.054e+02 | 
#    12500 | time 9.1e+02 sec | E recon = 8.370e-01 | Perplexity = 4.052e+02 | 
#    13000 | time 9.4e+02 sec | E recon = 8.269e-01 | Perplexity = 4.063e+02 | 
#    13500 | time 9.8e+02 sec | E recon = 8.283e-01 | Perplexity = 4.066e+02 | 
#    14000 | time 1.0e+03 sec | E recon = 8.233e-01 | Perplexity = 4.058e+02 | 
#    14500 | time 1.0e+03 sec | E recon = 8.239e-01 | Perplexity = 4.063e+02 | 
#    15000 | time 1.1e+03 sec | E recon = 8.223e-01 | Perplexity = 4.064e+02 | 
DONE (  1088.47 sec.) 


===================== -------------------------------------
[2023-04-04 15-06-48] Work is finished (1102.98 sec. total)


[2023-04-04 15-07-08] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "check"
Kind of task        : "gen"
=========================================


.... Loading "cifar10" dataset
DONE (     5.43 sec.) 


.... Loading "vae_vq" generator
DONE (     7.79 sec.) 


.... Loading "densenet" model
DONE (     2.65 sec.) 


.... Generate random images
Gen 25 random samples (time/sample 3.89e-01 sec)
Gen 25 random samples (time/sample 6.60e-05 sec)
Gen 25 random samples (time/sample 6.62e-05 sec)
Gen 25 random samples (time/sample 6.69e-05 sec)
Gen 25 random samples (time/sample 6.64e-05 sec)
DONE (    20.86 sec.) 


.... Reconstruct images from the dataset
Gen 25 embeddings     (time/sample 3.00e-03 sec)
Gen 25 embeddings     (time/sample 8.45e-05 sec)
Gen 25 embeddings     (time/sample 8.54e-05 sec)
Gen 25 embeddings     (time/sample 8.45e-05 sec)
Gen 25 embeddings     (time/sample 8.57e-05 sec)
DONE (    21.92 sec.) 


===================== -----------------------------------
[2023-04-04 15-08-07] Work is finished (58.70 sec. total)


[2023-04-04 15-08-24] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "check"
Kind of task        : "gen"
=========================================


.... Loading "cifar10" dataset
DONE (     5.36 sec.) 


.... Loading "gan_sn" generator
DONE (     8.25 sec.) 


.... Loading "densenet" model
DONE (     3.51 sec.) 


.... Generate random images
Gen 25 random samples (time/sample 4.04e-01 sec)
Gen 25 random samples (time/sample 7.09e-04 sec)
Gen 25 random samples (time/sample 7.09e-04 sec)
Gen 25 random samples (time/sample 7.09e-04 sec)
Gen 25 random samples (time/sample 7.11e-04 sec)
DONE (    21.72 sec.) 


===================== -----------------------------------
[2023-04-04 15-09-03] Work is finished (38.84 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-09-23] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "0"
=========================================


.... Loading "cifar10" dataset
DONE (     5.41 sec.) 


.... Loading "vae_vq" generator
DONE (     6.70 sec.) 


.... Loading "densenet" model
DONE (     2.20 sec.) 


.... Run AM for out class "0" (airplane)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.118e+01 | y  9.5114e-03
portfolio > m 7.0e+00 | t 1.235e+01 | y  1.1175e-02
portfolio > m 1.9e+01 | t 1.459e+01 | y  1.2927e-02
portfolio > m 2.5e+01 | t 1.575e+01 | y  1.7515e-02
portfolio > m 3.5e+01 | t 1.768e+01 | y  3.3218e-02
portfolio > m 7.2e+01 | t 2.527e+01 | y  1.2041e-01
portfolio > m 1.0e+02 | t 3.073e+01 | y  1.2041e-01 <<< DONE
Result: it 1.0e+02, t 3.1e+01, a 1.20409e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.852e+01 | y  8.6145e-03
protes > m 2.0e+01 | t 2.051e+01 | y  9.4524e-02
protes > m 3.0e+01 | t 2.249e+01 | y  9.3909e-01
protes > m 1.0e+02 | t 3.448e+01 | y  9.3909e-01 <<< DONE
Result: it 1.0e+02, t 3.5e+01, a 9.39085e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.63e-01 | y= 2.126331e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.76e-01 | y= 2.158204e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.96e-01 | y= 2.169217e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=3.15e-01 | y= 2.187972e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.35e-01 | y= 2.215507e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.54e-01 | y= 2.215507e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.73e-01 | y= 2.216865e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.92e-01 | y= 2.403476e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=4.12e-01 | y= 2.543395e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.31e-01 | y= 2.611188e-03 
Result: it 1.0e+02, t 8.0e-01, a 2.61120e-03
DONE (    67.80 sec.) 


===================== -----------------------------------
[2023-04-04 15-10-45] Work is finished (82.14 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-11-04] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "0"
=========================================


.... Loading "cifar10" dataset
DONE (     5.29 sec.) 


.... Loading "gan_sn" generator
DONE (     6.65 sec.) 


.... Loading "densenet" model
DONE (     2.03 sec.) 


.... Run AM for out class "0" (airplane)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.199e+01 | y  3.9691e-03
portfolio > m 2.0e+00 | t 1.204e+01 | y  9.8509e-03
portfolio > m 3.0e+00 | t 1.210e+01 | y  1.1141e-02
portfolio > m 4.0e+00 | t 1.216e+01 | y  5.2493e-01
portfolio > m 6.0e+00 | t 1.227e+01 | y  9.7956e-01
portfolio > m 2.4e+01 | t 1.326e+01 | y  9.7990e-01
portfolio > m 4.4e+01 | t 1.437e+01 | y  9.8008e-01
portfolio > m 1.0e+02 | t 1.753e+01 | y  9.8008e-01 <<< DONE
Result: it 1.0e+02, t 1.8e+01, a 9.80079e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.670e+00 | y  9.7057e-01
protes > m 2.0e+01 | t 9.264e+00 | y  9.7439e-01
protes > m 6.0e+01 | t 1.158e+01 | y  9.7707e-01
protes > m 9.0e+01 | t 1.332e+01 | y  9.7894e-01
protes > m 1.0e+02 | t 1.335e+01 | y  9.7894e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.78935e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.49e-01 | y= 4.530681e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.72e-01 | y= 4.889707e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.99e-01 | y= 5.376348e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.25e-01 | y= 5.934411e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.51e-01 | y= 5.934983e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.77e-01 | y= 5.934983e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.03e-01 | y= 5.934983e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.29e-01 | y= 5.934983e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.54e-01 | y= 6.101843e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.75e-01 | y= 6.108942e-03 
Result: it 1.0e+02, t 8.5e-01, a 6.10886e-03
DONE (    33.31 sec.) 


===================== -----------------------------------
[2023-04-04 15-11-52] Work is finished (47.32 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-12-11] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "1"
=========================================


.... Loading "cifar10" dataset
DONE (     5.29 sec.) 


.... Loading "vae_vq" generator
DONE (     6.43 sec.) 


.... Loading "densenet" model
DONE (     2.13 sec.) 


.... Run AM for out class "1" (automobile)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.048e+01 | y  1.0995e-02
portfolio > m 1.9e+01 | t 1.386e+01 | y  1.2384e-02
portfolio > m 2.5e+01 | t 1.501e+01 | y  1.6024e-02
portfolio > m 3.5e+01 | t 1.691e+01 | y  3.4479e-02
portfolio > m 1.0e+02 | t 2.947e+01 | y  3.4479e-02 <<< DONE
Result: it 1.0e+02, t 3.0e+01, a 3.44786e-02

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.015e+01 | y  8.2673e-03
protes > m 2.0e+01 | t 1.215e+01 | y  1.5353e-02
protes > m 4.0e+01 | t 1.629e+01 | y  2.6068e-02
protes > m 8.0e+01 | t 2.428e+01 | y  3.1313e-02
protes > m 1.0e+02 | t 2.630e+01 | y  3.1313e-02 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 3.13130e-02

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.37e-01 | y= 1.870416e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.49e-01 | y= 1.899059e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.69e-01 | y= 1.908939e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.88e-01 | y= 1.925814e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.08e-01 | y= 1.950511e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.27e-01 | y= 1.950511e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.46e-01 | y= 1.951750e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.65e-01 | y= 2.119631e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.85e-01 | y= 2.245466e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.04e-01 | y= 2.306455e-03 
Result: it 1.0e+02, t 6.1e-01, a 2.30646e-03
DONE (    58.00 sec.) 


===================== -----------------------------------
[2023-04-04 15-13-23] Work is finished (71.86 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-13-46] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "1"
=========================================


.... Loading "cifar10" dataset
DONE (    11.39 sec.) 


.... Loading "gan_sn" generator
DONE (     6.96 sec.) 


.... Loading "densenet" model
DONE (     2.07 sec.) 


.... Run AM for out class "1" (automobile)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.010e+01 | y  3.5502e-03
portfolio > m 2.0e+00 | t 1.015e+01 | y  7.4433e-02
portfolio > m 2.3e+01 | t 1.131e+01 | y  1.3111e-01
portfolio > m 2.9e+01 | t 1.163e+01 | y  9.7680e-01
portfolio > m 1.0e+02 | t 1.563e+01 | y  9.7680e-01 <<< DONE
Result: it 1.0e+02, t 1.6e+01, a 9.76796e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.709e+00 | y  2.7369e-02
protes > m 2.0e+01 | t 9.322e+00 | y  4.5087e-01
protes > m 3.0e+01 | t 9.953e+00 | y  9.7658e-01
protes > m 1.0e+02 | t 1.362e+01 | y  9.7662e-01 <<< DONE
Result: it 1.0e+02, t 1.4e+01, a 9.76616e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.52e-01 | y= 4.301549e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.77e-01 | y= 4.301549e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=3.04e-01 | y= 4.301549e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.32e-01 | y= 4.588999e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.59e-01 | y= 4.611002e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.86e-01 | y= 4.611002e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.12e-01 | y= 7.020078e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.39e-01 | y= 7.906736e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.65e-01 | y= 7.906736e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.89e-01 | y= 7.910996e-03 
Result: it 1.0e+02, t 8.7e-01, a 7.91093e-03
DONE (    31.67 sec.) 


===================== -----------------------------------
[2023-04-04 15-14-38] Work is finished (52.13 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-14-57] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "2"
=========================================


.... Loading "cifar10" dataset
DONE (     5.35 sec.) 


.... Loading "vae_vq" generator
DONE (    10.83 sec.) 


.... Loading "densenet" model
DONE (     2.15 sec.) 


.... Run AM for out class "2" (bird)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.276e+01 | y  9.0225e-03
portfolio > m 7.0e+00 | t 1.389e+01 | y  1.3809e-02
portfolio > m 1.9e+01 | t 1.607e+01 | y  1.6748e-02
portfolio > m 2.5e+01 | t 1.721e+01 | y  2.1322e-02
portfolio > m 3.5e+01 | t 1.909e+01 | y  3.5633e-02
portfolio > m 1.0e+02 | t 3.143e+01 | y  3.5633e-02 <<< DONE
Result: it 1.0e+02, t 3.2e+01, a 3.56333e-02

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.012e+01 | y  1.0601e-02
protes > m 2.0e+01 | t 1.209e+01 | y  2.0976e-02
protes > m 4.0e+01 | t 1.602e+01 | y  3.4063e-02
protes > m 1.0e+02 | t 2.589e+01 | y  6.6736e-02 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 6.67367e-02

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.37e-01 | y= 2.595896e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.49e-01 | y= 2.633607e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.69e-01 | y= 2.646489e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.88e-01 | y= 2.667735e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.06e-01 | y= 2.701733e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.25e-01 | y= 2.701733e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.44e-01 | y= 2.703359e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.62e-01 | y= 2.922904e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.81e-01 | y= 3.094696e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.00e-01 | y= 3.174706e-03 
Result: it 1.0e+02, t 6.6e-01, a 3.17471e-03
DONE (    65.47 sec.) 


===================== -----------------------------------
[2023-04-04 15-16-20] Work is finished (83.81 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-16-38] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "2"
=========================================


.... Loading "cifar10" dataset
DONE (     5.26 sec.) 


.... Loading "gan_sn" generator
DONE (     6.46 sec.) 


.... Loading "densenet" model
DONE (     2.06 sec.) 


.... Run AM for out class "2" (bird)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.915e+00 | y  7.0976e-03
portfolio > m 2.0e+00 | t 9.968e+00 | y  8.1311e-03
portfolio > m 3.0e+00 | t 1.003e+01 | y  1.9706e-02
portfolio > m 4.0e+00 | t 1.009e+01 | y  2.0942e-01
portfolio > m 2.6e+01 | t 1.133e+01 | y  8.1858e-01
portfolio > m 2.8e+01 | t 1.144e+01 | y  9.6936e-01
portfolio > m 4.8e+01 | t 1.259e+01 | y  9.7071e-01
portfolio > m 9.7e+01 | t 1.542e+01 | y  9.7708e-01
portfolio > m 1.0e+02 | t 1.559e+01 | y  9.7708e-01 <<< DONE
Result: it 1.0e+02, t 1.6e+01, a 9.77076e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.411e+00 | y  3.8006e-02
protes > m 2.0e+01 | t 9.015e+00 | y  4.8075e-02
protes > m 3.0e+01 | t 9.606e+00 | y  5.0757e-01
protes > m 5.0e+01 | t 1.078e+01 | y  9.3382e-01
protes > m 7.0e+01 | t 1.197e+01 | y  9.6827e-01
protes > m 1.0e+02 | t 1.318e+01 | y  9.6827e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.68271e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.53e-01 | y= 5.685799e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.77e-01 | y= 5.685799e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=3.04e-01 | y= 5.762410e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.31e-01 | y= 6.196506e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.58e-01 | y= 6.222547e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.85e-01 | y= 6.222547e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.11e-01 | y= 6.655809e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.38e-01 | y= 6.655809e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.65e-01 | y= 6.655809e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.88e-01 | y= 6.655809e-03 
Result: it 1.0e+02, t 8.7e-01, a 6.65577e-03
DONE (    31.23 sec.) 


===================== -----------------------------------
[2023-04-04 15-17-23] Work is finished (45.04 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-17-42] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "3"
=========================================


.... Loading "cifar10" dataset
DONE (     6.32 sec.) 


.... Loading "vae_vq" generator
DONE (     6.64 sec.) 


.... Loading "densenet" model
DONE (     2.07 sec.) 


.... Run AM for out class "3" (cat)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.040e+01 | y  2.4316e-02
portfolio > m 2.0e+00 | t 1.056e+01 | y  4.5187e-02
portfolio > m 3.0e+00 | t 1.073e+01 | y  7.1022e-01
portfolio > m 9.0e+00 | t 1.186e+01 | y  9.7326e-01
portfolio > m 3.0e+01 | t 1.571e+01 | y  9.8231e-01
portfolio > m 1.0e+02 | t 2.923e+01 | y  9.8231e-01 <<< DONE
Result: it 1.0e+02, t 2.9e+01, a 9.82312e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.014e+01 | y  9.7856e-01
protes > m 1.0e+02 | t 2.603e+01 | y  9.7926e-01 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 9.79261e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.44e-01 | y= 9.782853e-01 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.58e-01 | y= 9.782853e-01 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.78e-01 | y= 9.782853e-01 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.97e-01 | y= 9.782853e-01 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.17e-01 | y= 9.782853e-01 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.37e-01 | y= 9.782853e-01 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.56e-01 | y= 9.782853e-01 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.76e-01 | y= 9.782853e-01 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.95e-01 | y= 9.782853e-01 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.15e-01 | y= 9.782853e-01 
Result: it 1.0e+02, t 6.3e-01, a 9.78285e-01
DONE (    57.09 sec.) 


===================== -----------------------------------
[2023-04-04 15-18-54] Work is finished (72.18 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-19-27] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "3"
=========================================


.... Loading "cifar10" dataset
DONE (     6.73 sec.) 


.... Loading "gan_sn" generator
DONE (     6.75 sec.) 


.... Loading "densenet" model
DONE (     2.02 sec.) 


.... Run AM for out class "3" (cat)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.941e+00 | y  5.6575e-03
portfolio > m 2.0e+00 | t 9.992e+00 | y  8.6287e-03
portfolio > m 3.0e+00 | t 1.005e+01 | y  1.3896e-02
portfolio > m 4.0e+00 | t 1.011e+01 | y  3.6794e-02
portfolio > m 8.0e+00 | t 1.032e+01 | y  4.8659e-02
portfolio > m 9.0e+00 | t 1.037e+01 | y  9.1367e-01
portfolio > m 3.9e+01 | t 1.202e+01 | y  9.6105e-01
portfolio > m 4.3e+01 | t 1.223e+01 | y  9.7699e-01
portfolio > m 1.0e+02 | t 1.540e+01 | y  9.7699e-01 <<< DONE
Result: it 1.0e+02, t 1.5e+01, a 9.76990e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.389e+00 | y  8.4239e-01
protes > m 2.0e+01 | t 8.983e+00 | y  9.4974e-01
protes > m 4.0e+01 | t 1.014e+01 | y  9.6667e-01
protes > m 1.0e+02 | t 1.305e+01 | y  9.7798e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.77983e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.47e-01 | y= 5.968542e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.68e-01 | y= 5.968542e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.94e-01 | y= 5.991267e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.19e-01 | y= 6.426102e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.45e-01 | y= 6.453168e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.71e-01 | y= 6.453168e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=3.96e-01 | y= 6.668856e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.22e-01 | y= 6.668856e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.47e-01 | y= 6.668856e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.68e-01 | y= 6.668856e-03 
Result: it 1.0e+02, t 8.4e-01, a 6.66882e-03
DONE (    30.81 sec.) 


===================== -----------------------------------
[2023-04-04 15-20-13] Work is finished (46.33 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-20-31] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "4"
=========================================


.... Loading "cifar10" dataset
DONE (     5.33 sec.) 


.... Loading "vae_vq" generator
DONE (     6.38 sec.) 


.... Loading "densenet" model
DONE (     2.07 sec.) 


.... Run AM for out class "4" (deer)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.011e+01 | y  3.7484e-02
portfolio > m 1.0e+01 | t 1.177e+01 | y  3.9601e-02
portfolio > m 1.5e+01 | t 1.261e+01 | y  1.0833e-01
portfolio > m 3.8e+01 | t 1.712e+01 | y  5.8621e-01
portfolio > m 1.0e+02 | t 2.922e+01 | y  5.8621e-01 <<< DONE
Result: it 1.0e+02, t 2.9e+01, a 5.86210e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.002e+01 | y  8.3507e-01
protes > m 1.0e+02 | t 2.610e+01 | y  8.3507e-01 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 8.35068e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.37e-01 | y= 2.542643e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.51e-01 | y= 2.579930e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.71e-01 | y= 2.592682e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.91e-01 | y= 2.613302e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.10e-01 | y= 2.646890e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.30e-01 | y= 2.646890e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.49e-01 | y= 2.648501e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.68e-01 | y= 2.863992e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.88e-01 | y= 3.033014e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.07e-01 | y= 3.112223e-03 
Result: it 1.0e+02, t 6.1e-01, a 3.11223e-03
DONE (    57.40 sec.) 


===================== -----------------------------------
[2023-04-04 15-21-42] Work is finished (71.19 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-21-59] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "4"
=========================================


.... Loading "cifar10" dataset
DONE (     5.36 sec.) 


.... Loading "gan_sn" generator
DONE (     6.65 sec.) 


.... Loading "densenet" model
DONE (     2.09 sec.) 


.... Run AM for out class "4" (deer)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.022e+01 | y  9.5597e-01
portfolio > m 1.7e+01 | t 1.109e+01 | y  9.7463e-01
portfolio > m 7.2e+01 | t 1.419e+01 | y  9.7546e-01
portfolio > m 1.0e+02 | t 1.576e+01 | y  9.7546e-01 <<< DONE
Result: it 1.0e+02, t 1.6e+01, a 9.75464e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.401e+00 | y  9.7068e-01
protes > m 5.0e+01 | t 1.075e+01 | y  9.7648e-01
protes > m 6.0e+01 | t 1.133e+01 | y  9.7712e-01
protes > m 1.0e+02 | t 1.309e+01 | y  9.7712e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.77115e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.46e-01 | y= 4.979379e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.67e-01 | y= 4.979379e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.93e-01 | y= 5.011683e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.19e-01 | y= 5.386149e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.45e-01 | y= 5.409608e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.71e-01 | y= 5.409608e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=3.96e-01 | y= 5.653875e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.22e-01 | y= 5.653875e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.48e-01 | y= 5.655294e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.68e-01 | y= 5.655294e-03 
Result: it 1.0e+02, t 8.4e-01, a 5.65527e-03
DONE (    30.94 sec.) 


===================== -----------------------------------
[2023-04-04 15-22-44] Work is finished (45.07 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-23-01] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "5"
=========================================


.... Loading "cifar10" dataset
DONE (     5.31 sec.) 


.... Loading "vae_vq" generator
DONE (     6.21 sec.) 


.... Loading "densenet" model
DONE (     2.18 sec.) 


.... Run AM for out class "5" (dog)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.506e+01 | y  1.7723e-02
portfolio > m 2.0e+00 | t 1.522e+01 | y  9.0162e-01
portfolio > m 4.0e+00 | t 1.558e+01 | y  9.7592e-01
portfolio > m 8.0e+00 | t 1.638e+01 | y  9.8106e-01
portfolio > m 2.2e+01 | t 1.898e+01 | y  9.8134e-01
portfolio > m 8.3e+01 | t 3.082e+01 | y  9.8156e-01
portfolio > m 9.3e+01 | t 3.256e+01 | y  9.8190e-01
portfolio > m 1.0e+02 | t 3.410e+01 | y  9.8190e-01 <<< DONE
Result: it 1.0e+02, t 3.4e+01, a 9.81902e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 9.834e+00 | y  9.7417e-01
protes > m 2.0e+01 | t 1.185e+01 | y  9.8045e-01
protes > m 4.0e+01 | t 1.595e+01 | y  9.8333e-01
protes > m 1.0e+02 | t 2.603e+01 | y  9.8333e-01 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 9.83326e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.40e-01 | y= 5.460304e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.53e-01 | y= 5.536791e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.72e-01 | y= 5.554637e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.91e-01 | y= 5.616030e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.10e-01 | y= 5.616030e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.29e-01 | y= 5.726031e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.48e-01 | y= 5.726031e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.66e-01 | y= 6.059709e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.86e-01 | y= 6.588618e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.10e-01 | y= 6.588618e-03 
Result: it 1.0e+02, t 6.1e-01, a 6.58862e-03
DONE (    62.36 sec.) 


===================== -----------------------------------
[2023-04-04 15-24-17] Work is finished (76.06 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-24-35] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "5"
=========================================


.... Loading "cifar10" dataset
DONE (     5.32 sec.) 


.... Loading "gan_sn" generator
DONE (     6.60 sec.) 


.... Loading "densenet" model
DONE (     2.10 sec.) 


.... Run AM for out class "5" (dog)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.010e+01 | y  6.8983e-03
portfolio > m 2.0e+00 | t 1.015e+01 | y  8.0664e-03
portfolio > m 3.0e+00 | t 1.021e+01 | y  1.5030e-02
portfolio > m 4.0e+00 | t 1.027e+01 | y  3.3637e-02
portfolio > m 8.0e+00 | t 1.048e+01 | y  4.3602e-02
portfolio > m 2.3e+01 | t 1.131e+01 | y  6.1470e-02
portfolio > m 5.4e+01 | t 1.303e+01 | y  2.9503e-01
portfolio > m 8.0e+01 | t 1.451e+01 | y  5.1303e-01
portfolio > m 1.0e+02 | t 1.562e+01 | y  5.1303e-01 <<< DONE
Result: it 1.0e+02, t 1.6e+01, a 5.13029e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.477e+00 | y  8.9033e-02
protes > m 2.0e+01 | t 9.089e+00 | y  8.9420e-01
protes > m 3.0e+01 | t 9.688e+00 | y  9.4039e-01
protes > m 5.0e+01 | t 1.088e+01 | y  9.7842e-01
protes > m 1.0e+02 | t 1.330e+01 | y  9.7842e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.78420e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.50e-01 | y= 4.692843e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.74e-01 | y= 4.692843e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=3.01e-01 | y= 4.736263e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.28e-01 | y= 5.092819e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.55e-01 | y= 5.114456e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.81e-01 | y= 5.114456e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.07e-01 | y= 5.326287e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.34e-01 | y= 5.326287e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.60e-01 | y= 5.326287e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.84e-01 | y= 5.326287e-03 
Result: it 1.0e+02, t 8.8e-01, a 5.32626e-03
DONE (    31.35 sec.) 


===================== -----------------------------------
[2023-04-04 15-25-20] Work is finished (45.37 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-25-38] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "6"
=========================================


.... Loading "cifar10" dataset
DONE (     6.04 sec.) 


.... Loading "vae_vq" generator
DONE (     8.07 sec.) 


.... Loading "densenet" model
DONE (     2.03 sec.) 


.... Run AM for out class "6" (frog)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.406e+01 | y  1.0307e-02
portfolio > m 7.0e+00 | t 1.524e+01 | y  1.1834e-02
portfolio > m 1.9e+01 | t 1.746e+01 | y  5.1623e-02
portfolio > m 5.0e+01 | t 2.329e+01 | y  4.6135e-01
portfolio > m 5.2e+01 | t 2.363e+01 | y  9.6915e-01
portfolio > m 1.0e+02 | t 3.265e+01 | y  9.6915e-01 <<< DONE
Result: it 1.0e+02, t 3.3e+01, a 9.69152e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.165e+01 | y  9.6213e-03
protes > m 2.0e+01 | t 1.363e+01 | y  7.8217e-01
protes > m 1.0e+02 | t 2.743e+01 | y  7.8217e-01 <<< DONE
Result: it 1.0e+02, t 2.7e+01, a 7.82167e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.38e-01 | y= 2.629683e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.51e-01 | y= 2.667948e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.71e-01 | y= 2.681105e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.91e-01 | y= 2.702982e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.11e-01 | y= 2.736443e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.31e-01 | y= 2.736443e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.50e-01 | y= 2.738052e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.70e-01 | y= 2.959791e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.89e-01 | y= 3.128345e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.08e-01 | y= 3.208998e-03 
Result: it 1.0e+02, t 6.1e-01, a 3.20901e-03
DONE (    62.45 sec.) 


===================== -----------------------------------
[2023-04-04 15-26-56] Work is finished (78.61 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-27-14] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "6"
=========================================


.... Loading "cifar10" dataset
DONE (     5.34 sec.) 


.... Loading "gan_sn" generator
DONE (     6.80 sec.) 


.... Loading "densenet" model
DONE (     2.09 sec.) 


.... Run AM for out class "6" (frog)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.332e+01 | y  4.4235e-03
portfolio > m 2.0e+00 | t 1.337e+01 | y  8.1988e-03
portfolio > m 3.0e+00 | t 1.343e+01 | y  1.0622e-02
portfolio > m 4.0e+00 | t 1.349e+01 | y  3.1767e-02
portfolio > m 5.0e+00 | t 1.354e+01 | y  9.8089e-01
portfolio > m 1.0e+02 | t 1.897e+01 | y  9.8089e-01 <<< DONE
Result: it 1.0e+02, t 1.9e+01, a 9.80891e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.474e+01 | y  8.2651e-01
protes > m 2.0e+01 | t 1.534e+01 | y  9.7852e-01
protes > m 3.0e+01 | t 1.592e+01 | y  9.7967e-01
protes > m 1.0e+02 | t 1.945e+01 | y  9.7967e-01 <<< DONE
Result: it 1.0e+02, t 1.9e+01, a 9.79673e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.50e-01 | y= 9.759977e-01 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.73e-01 | y= 9.772295e-01 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.99e-01 | y= 9.772295e-01 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.25e-01 | y= 9.772295e-01 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.51e-01 | y= 9.772295e-01 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.77e-01 | y= 9.772295e-01 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.03e-01 | y= 9.772295e-01 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.29e-01 | y= 9.772295e-01 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.55e-01 | y= 9.772295e-01 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.76e-01 | y= 9.772295e-01 
Result: it 1.0e+02, t 8.6e-01, a 9.77229e-01
DONE (    40.63 sec.) 


===================== -----------------------------------
[2023-04-04 15-28-09] Work is finished (54.87 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-28-29] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "7"
=========================================


.... Loading "cifar10" dataset
DONE (     5.52 sec.) 


.... Loading "vae_vq" generator
DONE (     9.04 sec.) 


.... Loading "densenet" model
DONE (     2.06 sec.) 


.... Run AM for out class "7" (horse)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.901e+00 | y  8.9778e-03
portfolio > m 7.0e+00 | t 1.102e+01 | y  7.3902e-02
portfolio > m 2.0e+01 | t 1.335e+01 | y  4.3861e-01
portfolio > m 3.5e+01 | t 1.619e+01 | y  5.5680e-01
portfolio > m 3.9e+01 | t 1.687e+01 | y  8.2230e-01
portfolio > m 6.5e+01 | t 2.185e+01 | y  9.0589e-01
portfolio > m 7.7e+01 | t 2.407e+01 | y  9.5527e-01
portfolio > m 1.0e+02 | t 2.851e+01 | y  9.5527e-01 <<< DONE
Result: it 1.0e+02, t 2.9e+01, a 9.55275e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.109e+01 | y  1.1341e-02
protes > m 2.0e+01 | t 1.315e+01 | y  8.8823e-01
protes > m 4.0e+01 | t 1.712e+01 | y  9.6445e-01
protes > m 1.0e+02 | t 2.706e+01 | y  9.6445e-01 <<< DONE
Result: it 1.0e+02, t 2.7e+01, a 9.64453e-01

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.38e-01 | y= 2.293031e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.49e-01 | y= 2.327241e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.68e-01 | y= 2.338902e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.87e-01 | y= 2.357670e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.06e-01 | y= 2.388836e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.25e-01 | y= 2.388836e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.44e-01 | y= 2.390293e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.64e-01 | y= 2.588250e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.83e-01 | y= 2.747180e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.02e-01 | y= 2.820165e-03 
Result: it 1.0e+02, t 6.0e-01, a 2.82017e-03
DONE (    57.89 sec.) 


===================== -----------------------------------
[2023-04-04 15-29-44] Work is finished (74.55 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-30-01] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "7"
=========================================


.... Loading "cifar10" dataset
DONE (     5.26 sec.) 


.... Loading "gan_sn" generator
DONE (     6.62 sec.) 


.... Loading "densenet" model
DONE (     2.07 sec.) 


.... Run AM for out class "7" (horse)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.943e+00 | y  5.1070e-03
portfolio > m 2.0e+00 | t 9.996e+00 | y  8.2493e-03
portfolio > m 3.0e+00 | t 1.006e+01 | y  4.9000e-01
portfolio > m 8.0e+00 | t 1.033e+01 | y  6.3341e-01
portfolio > m 2.0e+01 | t 1.103e+01 | y  8.2556e-01
portfolio > m 3.5e+01 | t 1.187e+01 | y  9.7391e-01
portfolio > m 9.5e+01 | t 1.538e+01 | y  9.7406e-01
portfolio > m 1.0e+02 | t 1.567e+01 | y  9.7406e-01 <<< DONE
Result: it 1.0e+02, t 1.6e+01, a 9.74057e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.370e+00 | y  2.8102e-02
protes > m 2.0e+01 | t 8.984e+00 | y  4.9563e-02
protes > m 3.0e+01 | t 9.583e+00 | y  7.1589e-02
protes > m 4.0e+01 | t 1.018e+01 | y  9.6894e-01
protes > m 1.0e+02 | t 1.318e+01 | y  9.6894e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.68940e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.48e-01 | y= 5.389677e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.71e-01 | y= 5.389677e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.97e-01 | y= 1.010569e-02 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.23e-01 | y= 1.010569e-02 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.50e-01 | y= 1.010569e-02 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.76e-01 | y= 2.240156e-02 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.02e-01 | y= 2.240156e-02 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.29e-01 | y= 2.600485e-02 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.55e-01 | y= 2.612002e-02 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.77e-01 | y= 2.612002e-02 
Result: it 1.0e+02, t 8.5e-01, a 2.61197e-02
DONE (    31.20 sec.) 


===================== -----------------------------------
[2023-04-04 15-30-46] Work is finished (45.15 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-31-04] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "8"
=========================================


.... Loading "cifar10" dataset
DONE (     5.36 sec.) 


.... Loading "vae_vq" generator
DONE (     7.52 sec.) 


.... Loading "densenet" model
DONE (     2.74 sec.) 


.... Run AM for out class "8" (ship)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.856e+00 | y  9.3866e-03
portfolio > m 7.0e+00 | t 1.098e+01 | y  1.0658e-02
portfolio > m 1.9e+01 | t 1.317e+01 | y  1.2440e-02
portfolio > m 2.5e+01 | t 1.430e+01 | y  1.6441e-02
portfolio > m 3.5e+01 | t 1.618e+01 | y  3.2446e-02
portfolio > m 1.0e+02 | t 2.856e+01 | y  3.2446e-02 <<< DONE
Result: it 1.0e+02, t 2.9e+01, a 3.24455e-02

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 1.001e+01 | y  8.3224e-03
protes > m 2.0e+01 | t 1.199e+01 | y  1.5772e-02
protes > m 3.0e+01 | t 1.397e+01 | y  1.6655e-02
protes > m 4.0e+01 | t 1.595e+01 | y  2.6362e-02
protes > m 8.0e+01 | t 2.387e+01 | y  3.3272e-02
protes > m 1.0e+02 | t 2.587e+01 | y  3.3272e-02 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 3.32719e-02

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.39e-01 | y= 1.959352e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.52e-01 | y= 1.989181e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.71e-01 | y= 1.999479e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.91e-01 | y= 2.017094e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.10e-01 | y= 2.042703e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.29e-01 | y= 2.042703e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.48e-01 | y= 2.043985e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.67e-01 | y= 2.218572e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.86e-01 | y= 2.348913e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.05e-01 | y= 2.412321e-03 
Result: it 1.0e+02, t 6.1e-01, a 2.41232e-03
DONE (    56.79 sec.) 


===================== -----------------------------------
[2023-04-04 15-32-16] Work is finished (72.42 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-32-34] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "8"
=========================================


.... Loading "cifar10" dataset
DONE (     5.29 sec.) 


.... Loading "gan_sn" generator
DONE (     7.36 sec.) 


.... Loading "densenet" model
DONE (     2.07 sec.) 


.... Run AM for out class "8" (ship)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.626e+00 | y  3.6933e-03
portfolio > m 2.0e+00 | t 9.680e+00 | y  1.0307e-02
portfolio > m 4.0e+00 | t 9.797e+00 | y  3.9844e-02
portfolio > m 2.3e+01 | t 1.086e+01 | y  6.4153e-02
portfolio > m 2.5e+01 | t 1.098e+01 | y  1.3311e-01
portfolio > m 3.6e+01 | t 1.160e+01 | y  9.7178e-01
portfolio > m 9.5e+01 | t 1.501e+01 | y  9.7182e-01
portfolio > m 1.0e+02 | t 1.530e+01 | y  9.7182e-01 <<< DONE
Result: it 1.0e+02, t 1.5e+01, a 9.71819e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 9.326e+00 | y  9.5335e-01
protes > m 4.0e+01 | t 1.109e+01 | y  9.7252e-01
protes > m 1.0e+02 | t 1.400e+01 | y  9.7252e-01 <<< DONE
Result: it 1.0e+02, t 1.4e+01, a 9.72521e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.48e-01 | y= 7.790734e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.69e-01 | y= 7.896563e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=2.95e-01 | y= 8.858209e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.21e-01 | y= 8.858209e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.47e-01 | y= 9.135932e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.72e-01 | y= 9.135932e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=3.98e-01 | y= 9.135932e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.24e-01 | y= 9.764465e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.49e-01 | y= 1.046511e-02 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.69e-01 | y= 1.184261e-02 
Result: it 1.0e+02, t 8.4e-01, a 1.18426e-02
DONE (    31.78 sec.) 


===================== -----------------------------------
[2023-04-04 15-33-20] Work is finished (46.51 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-33-38] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "vae_vq"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "9"
=========================================


.... Loading "cifar10" dataset
DONE (     5.17 sec.) 


.... Loading "vae_vq" generator
DONE (     6.37 sec.) 


.... Loading "densenet" model
DONE (     2.08 sec.) 


.... Run AM for out class "9" (truck)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 9.865e+00 | y  8.6228e-01
portfolio > m 4.2e+01 | t 1.743e+01 | y  8.8096e-01
portfolio > m 1.0e+02 | t 2.870e+01 | y  8.8096e-01 <<< DONE
Result: it 1.0e+02, t 2.9e+01, a 8.80963e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 9.995e+00 | y  9.2113e-03
protes > m 2.0e+01 | t 1.195e+01 | y  1.6140e-02
protes > m 3.0e+01 | t 1.396e+01 | y  2.2812e-02
protes > m 4.0e+01 | t 1.597e+01 | y  4.4896e-02
protes > m 1.0e+02 | t 2.603e+01 | y  4.4896e-02 <<< DONE
Result: it 1.0e+02, t 2.6e+01, a 4.48960e-02

Optimization with "TTOpt" method:
ttopt-64d  | evals=1.20e+01+6.00e+00 | t_cur=1.38e-01 | y= 1.973147e-03 
ttopt-64d  | evals=2.10e+01+1.50e+01 | t_cur=2.52e-01 | y= 2.003168e-03 
ttopt-64d  | evals=3.00e+01+2.40e+01 | t_cur=2.72e-01 | y= 2.013526e-03 
ttopt-64d  | evals=3.90e+01+3.30e+01 | t_cur=2.91e-01 | y= 2.034256e-03 
ttopt-64d  | evals=4.80e+01+4.20e+01 | t_cur=3.11e-01 | y= 2.057281e-03 
ttopt-64d  | evals=5.70e+01+5.10e+01 | t_cur=3.30e-01 | y= 2.057281e-03 
ttopt-64d  | evals=6.60e+01+6.00e+01 | t_cur=3.49e-01 | y= 2.058728e-03 
ttopt-64d  | evals=7.50e+01+6.90e+01 | t_cur=3.68e-01 | y= 2.246793e-03 
ttopt-64d  | evals=8.40e+01+7.80e+01 | t_cur=3.88e-01 | y= 2.372819e-03 
ttopt-64d  | evals=9.30e+01+8.50e+01 | t_cur=4.07e-01 | y= 2.436546e-03 
Result: it 1.0e+02, t 6.1e-01, a 2.43655e-03
DONE (    56.87 sec.) 


===================== -----------------------------------
[2023-04-04 15-34-48] Work is finished (70.50 sec. total)


/home/a.chertkov/.conda/neural_tensor_train/lib/python3.8/site-packages/nevergrad/optimization/differentialevolution.py:107: InefficientSettingsWarning: DE algorithms are inefficient with budget < 60
  warnings.warn(
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2023-04-04 15-35-07] Computations (cuda)
===================== -------------------
Data                : "cifar10"
Gen                 : "gan_sn"
Model               : "densenet"
Task                : "am"
Kind of task        : "class"
Target class        : "9"
=========================================


.... Loading "cifar10" dataset
DONE (     5.41 sec.) 


.... Loading "gan_sn" generator
DONE (     6.78 sec.) 


.... Loading "densenet" model
DONE (     2.12 sec.) 


.... Run AM for out class "9" (truck)

Optimization with "Portfolio" method:
portfolio > m 1.0e+00 | t 1.167e+01 | y  3.6359e-03
portfolio > m 2.0e+00 | t 1.172e+01 | y  8.5619e-01
portfolio > m 1.1e+01 | t 1.221e+01 | y  9.7054e-01
portfolio > m 1.5e+01 | t 1.243e+01 | y  9.7551e-01
portfolio > m 1.0e+02 | t 1.718e+01 | y  9.7551e-01 <<< DONE
Result: it 1.0e+02, t 1.7e+01, a 9.75512e-01

Optimization with "PROTES" method:
protes > m 1.0e+01 | t 8.582e+00 | y  9.0311e-01
protes > m 3.0e+01 | t 9.765e+00 | y  9.7764e-01
protes > m 6.0e+01 | t 1.152e+01 | y  9.7950e-01
protes > m 1.0e+02 | t 1.330e+01 | y  9.7950e-01 <<< DONE
Result: it 1.0e+02, t 1.3e+01, a 9.79496e-01

Optimization with "TTOpt" method:
ttopt-128d | evals=1.20e+01+6.00e+00 | t_cur=1.56e-01 | y= 4.393104e-03 
ttopt-128d | evals=2.10e+01+1.50e+01 | t_cur=2.78e-01 | y= 4.393104e-03 
ttopt-128d | evals=3.00e+01+2.40e+01 | t_cur=3.04e-01 | y= 4.393104e-03 
ttopt-128d | evals=3.90e+01+3.30e+01 | t_cur=3.30e-01 | y= 4.676516e-03 
ttopt-128d | evals=4.80e+01+4.20e+01 | t_cur=3.56e-01 | y= 4.699055e-03 
ttopt-128d | evals=5.70e+01+5.10e+01 | t_cur=3.81e-01 | y= 4.699055e-03 
ttopt-128d | evals=6.60e+01+6.00e+01 | t_cur=4.07e-01 | y= 7.555198e-03 
ttopt-128d | evals=7.50e+01+6.90e+01 | t_cur=4.33e-01 | y= 8.551560e-03 
ttopt-128d | evals=8.40e+01+7.80e+01 | t_cur=4.59e-01 | y= 8.551560e-03 
ttopt-128d | evals=9.20e+01+8.60e+01 | t_cur=5.80e-01 | y= 8.560751e-03 
Result: it 1.0e+02, t 8.9e-01, a 8.56068e-03
DONE (    32.72 sec.) 


===================== -----------------------------------
[2023-04-04 15-35-54] Work is finished (47.06 sec. total)


